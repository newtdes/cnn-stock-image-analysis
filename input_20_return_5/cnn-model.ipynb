{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "use_random_split = False\n",
    "use_dataparallel = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13c92bcd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All files are here\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'utils'))\n",
    "if use_gpu:\n",
    "    from gpu_tools import select_gpu\n",
    "    from gpu_tools import query_gpu\n",
    "    os.environ[\"CUDA_INVISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We write a function to convert the images folder to .dat type\n",
    "def images_to_dat(images_folder, output_file):\n",
    "    with open(output_file, \"wb\") as dat_file:\n",
    "        for filename in sorted([f for f in os.listdir(images_folder) if f != \"output.csv\" and f.endswith(\".jpeg\")], key = lambda x: int(x.split('_')[0])):\n",
    "        # for filename in os.listdir(images_folder):\n",
    "            if filename.endswith(\".jpeg\"):\n",
    "            # Read the image file\n",
    "                img_path = os.path.join(images_folder, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Failed to load {filename}\")\n",
    "                    continue \n",
    "                img = img > 32\n",
    "                img = img.astype(np.uint8) * 255\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                img_array = np.array(img)\n",
    "\n",
    "                # Flatten the image and convert to bytes, for easier storage\n",
    "                image_bytes = img_array.flatten().tobytes()\n",
    "\n",
    "                # Write the image data and store that in .dat file\n",
    "                dat_file.write(image_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "images_to_dat(\"training_data\", \"training_data.dat\")\n",
    "images_to_dat(\"validation_data\", \"validation_data.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287102, 64, 60)\n"
     ]
    }
   ],
   "source": [
    "# We then create an array of images\n",
    "# This is a breakthrough, at least for me, since I cannot do anything to store a large amount like that\n",
    "# to a list, until I find np.memmap\n",
    "training_data = []\n",
    "training_data.append(np.memmap(\"training_data.dat\", dtype = np.uint8, mode = 'r').reshape(\n",
    "                        (-1, 64, 60))) # since it is 60 x 60 images\n",
    "training_data = np.concatenate(training_data)\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123045, 64, 60)\n"
     ]
    }
   ],
   "source": [
    "# Similarly, for validation_data\n",
    "validation_data = []\n",
    "validation_data.append(np.memmap(\"validation_data.dat\", dtype = np.uint8, mode = 'r').reshape(\n",
    "    (-1, 64, 60)))\n",
    "validation_data = np.concatenate(validation_data)\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For output\n",
    "\n",
    "def extract_outputs(images_folder):\n",
    "    outputs = []\n",
    "\n",
    "    for filename in sorted([f for f in os.listdir(images_folder) if f != \"output.csv\" and f.endswith(\".jpeg\")], key = lambda x: int(x.split('_')[0])):\n",
    "        if filename.endswith('.jpeg'):  # Adjust based on file extension\n",
    "            # Split the filename to extract the output value\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) > 4:  # Ensure there are enough parts\n",
    "                output_value = parts[-1].split('.')[0]  # Get the last part before the extension\n",
    "                outputs.append(int(output_value))  # Convert to integer and store\n",
    "            \n",
    "    return outputs\n",
    "\n",
    "def save_outputs_to_csv(images_folder, output_file):\n",
    "    outputs = extract_outputs(images_folder)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'output': outputs})\n",
    "    \n",
    "    # Save DataFrame as a Feather file\n",
    "    df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_outputs_to_csv(\"training_data\", \"training_outputs.csv\")\n",
    "save_outputs_to_csv(\"validation_data\", \"validation_outputs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Shape: (287102, 1)\n",
      "Validation Label Shape: (123045, 1)\n"
     ]
    }
   ],
   "source": [
    "training_outputs = pd.read_csv(\"training_outputs.csv\")\n",
    "training_outputs = training_outputs.drop(training_outputs.columns[0], axis = 1)\n",
    "print(f\"Training Label Shape: {training_outputs.shape}\")\n",
    "validation_outputs = pd.read_csv(\"validation_outputs.csv\")\n",
    "validation_outputs = validation_outputs.drop(validation_outputs.columns[0], axis = 1)\n",
    "print(f\"Validation Label Shape: {validation_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change both to numpy array\n",
    "training_outputs = training_outputs.to_numpy()\n",
    "validation_outputs = validation_outputs.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squeeze shape of training outputs: (287102,)\n",
      "Squeeze shape of validation outputs: (123045,)\n"
     ]
    }
   ],
   "source": [
    "# Try Squeeze Shape\n",
    "training_outputs = training_outputs.squeeze()\n",
    "validation_outputs = validation_outputs.squeeze()\n",
    "print(f\"Squeeze shape of training outputs: {training_outputs.shape}\")\n",
    "print(f\"Squeeze shape of validation outputs: {validation_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy array of training data and validation data\n",
    "# to Tensor\n",
    "training_dataset = MyDataset(training_data, training_outputs)\n",
    "validation_dataset = MyDataset(validation_data, validation_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need DataLoader to do batch training\n",
    "train_dataloader = DataLoader(training_dataset, batch_size = 128, shuffle = True, pin_memory = True)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size = 256, shuffle = False, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we come to models\n",
    "# Initialization of weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from models import baseline\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "export_onnx = True\n",
    "net = baseline.Net().to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "if export_onnx:\n",
    "    import torch.onnx\n",
    "    x = torch.randn([1,1,64,60]).to(device)\n",
    "    torch.onnx.export(net,               # model being run\n",
    "                      x,                         # model input (or a tuple for multiple inputs)\n",
    "                      \"../cnn_baseline.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                      export_params=False,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=10,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=False,  # whether to execute constant folding for optimization\n",
    "                      input_names = ['input_images'],   # the model's input names\n",
    "                      output_names = ['output_prob'], # the model's output names\n",
    "                      dynamic_axes={'input_images' : {0 : 'batch_size'},    # variable length axes\n",
    "                                     'output_prob' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.weight : torch.Size([64, 1, 5, 3])\n",
      "layer1.0.bias : torch.Size([64])\n",
      "layer1.1.weight : torch.Size([64])\n",
      "layer1.1.bias : torch.Size([64])\n",
      "layer2.0.weight : torch.Size([128, 64, 5, 3])\n",
      "layer2.0.bias : torch.Size([128])\n",
      "layer2.1.weight : torch.Size([128])\n",
      "layer2.1.bias : torch.Size([128])\n",
      "layer3.0.weight : torch.Size([256, 128, 5, 3])\n",
      "layer3.0.bias : torch.Size([256])\n",
      "layer3.1.weight : torch.Size([256])\n",
      "layer3.1.bias : torch.Size([256])\n",
      "fc1.1.weight : torch.Size([2, 46080])\n",
      "fc1.1.bias : torch.Size([2])\n",
      "total_parameters : 708866\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for name, parameters in net.named_parameters():\n",
    "    print(name, ':', parameters.size())\n",
    "    count += parameters.numel()\n",
    "print('total_parameters : {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs = 36.21961728G\n",
      "Params = 0.708866M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile as thop_profile\n",
    "\n",
    "flops, params = thop_profile(net, inputs=(next(iter(train_dataloader))[0].to(device),))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-11-16 01:11:46 11951:563953 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference        11.53%      54.579ms       100.00%     473.470ms     473.470ms             1  \n",
      "                    aten::reshape         0.00%       8.000us         0.01%      28.000us      14.000us             2  \n",
      "                       aten::view         0.00%      20.000us         0.00%      20.000us      10.000us             2  \n",
      "                     aten::conv2d         0.00%      18.000us        50.23%     237.847ms      79.282ms             3  \n",
      "                aten::convolution         0.02%      72.000us        50.23%     237.829ms      79.276ms             3  \n",
      "               aten::_convolution         0.01%      44.000us        50.22%     237.757ms      79.252ms             3  \n",
      "         aten::mkldnn_convolution        50.20%     237.666ms        50.21%     237.713ms      79.238ms             3  \n",
      "                      aten::empty         0.02%      91.000us         0.02%      91.000us       2.844us            32  \n",
      "                aten::as_strided_         0.00%      17.000us         0.00%      17.000us       5.667us             3  \n",
      "                    aten::resize_         0.00%       5.000us         0.00%       5.000us       1.667us             3  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 473.470ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-11-16 01:11:46 11951:563953 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-11-16 01:11:46 11951:563953 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "inputs = next(iter(train_dataloader))[0].to(device)\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        net(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"../trace.json\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training stage\n",
    "def train_loop(dataloader, net, loss_fn, optimizer):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.train()\n",
    "    \n",
    "    with tqdm(dataloader) as t:\n",
    "        for batch, (X, y) in enumerate(t):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = net(X)\n",
    "            loss = loss_fn(y_pred, y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n",
    "            current += len(X)\n",
    "            t.set_postfix({'running_loss':running_loss})\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(dataloader, net, loss_fn):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader) as t:\n",
    "            for batch, (X, y) in enumerate(t):\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = net(X)\n",
    "                loss = loss_fn(y_pred, y.long())\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_loss = (len(X) * running_loss + loss.item() * current) / (len(X) + current)\n",
    "                current += len(X)\n",
    "            \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu and use_dataparallel and 'DataParallel' not in str(type(net)):\n",
    "    net = net.to(device)\n",
    "    net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "start_epoch = 0\n",
    "min_val_loss = 1e9\n",
    "last_min_ind = -1\n",
    "early_stopping_epoch = 5\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2243 [00:20<37:18,  1.01s/it, running_loss=1.14]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m0.2\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m train_loss \u001b[39m=\u001b[39m train_loop(train_dataloader, net, loss_fn, optimizer)\n\u001b[1;32m      8\u001b[0m val_loss \u001b[39m=\u001b[39m val_loop(val_dataloader, net, loss_fn)\n\u001b[1;32m      9\u001b[0m tb\u001b[39m.\u001b[39madd_histogram(\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m, train_loss, t)\n",
      "Cell \u001b[0;32mIn[85], line 12\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, net, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m y_pred \u001b[39m=\u001b[39m net(X)\n\u001b[1;32m     13\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y\u001b[39m.\u001b[39mlong())\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github Repo/cnn-stock-image-analysis/input_20_return_5/models/baseline.py:33\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     32\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m60\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m     34\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m     35\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now().strftime('%Y%m%d_%H:%M:%S')\n",
    "# os.mkdir('../pt'+os.sep+start_time)\n",
    "epochs = 100\n",
    "for t in range(start_epoch, epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    time.sleep(0.2)\n",
    "    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer)\n",
    "    val_loss = val_loop(val_dataloader, net, loss_fn)\n",
    "    tb.add_histogram(\"train_loss\", train_loss, t)\n",
    "    torch.save(net, '../pt'+os.sep+start_time+os.sep+'baseline_epoch_{}_train_{:5f}_val_{:5f}.pt'.format(t, train_loss, val_loss)) \n",
    "    if val_loss < min_val_loss:\n",
    "        last_min_ind = t\n",
    "        min_val_loss = val_loss\n",
    "    elif t - last_min_ind >= early_stopping_epoch:\n",
    "        break\n",
    "\n",
    "print('Done!')\n",
    "print('Best epoch: {}, val_loss: {}'.format(last_min_ind, min_val_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
